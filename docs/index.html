<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Home</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Home</h1>

    



    


    <h3> </h3>










    




    <section>
        <article><blockquote>
<p><strong>NOTE</strong> This branch contains the work-in-progress code for azbak 2.0.
Looking for the stable <a href="https://github.com/EgoAleSum/azbak.js/tree/v1.0.0">1.0.0 release</a>?</p>
</blockquote>
<h1>azbak</h1><p>Command-line utility and Node.js module to backup a file or a stream to Azure Blob Storage.</p>
<p>Features:</p>
<ul>
<li>Fully stream-based</li>
<li>The CLI supports piping input from a stream or reading from a file on disk</li>
<li>Automatically chunks files/streams bigger than the maximum blob size (~4.8 TB) into multiple blobs</li>
<li>Cross-platform</li>
<li>Small memory footprint</li>
</ul>
<h1>Command-line tool</h1><h2>Installation</h2><p>azbak requires Node.js version 4.0 or higher and NPM.</p>
<p>You can install the application from NPM:</p>
<pre class="prettyprint source lang-sh"><code># Depending on your system, you may need to run this as root or add &quot;sudo&quot;
$ npm install --global azbak</code></pre><h2>Usage</h2><p>Command reference:</p>
<pre class="prettyprint source"><code>$ azbak [options] &lt;input> &lt;destinationPath></code></pre><h3>Authentication</h3><p>You need to authenticate against Azure Blob Storage using a storage account name and an access key. azbak supports passing these values in the same way as the official Azure CLI, using environmental variables <strong><code>AZURE_STORAGE_ACCOUNT</code></strong> and <strong><code>AZURE_STORAGE_ACCESS_KEY</code></strong>.</p>
<h3>Arguments</h3><p><strong><code>input</code></strong> is either:</p>
<ul>
<li>The path of a local file to upload (e.g. <code>/path/to/file.jpg</code>)</li>
<li>A dash (<strong><code>-</code></strong>) to read from stdin</li>
</ul>
<p><strong><code>destinationPath</code></strong> is the path inside the Azure Blob Storage account used as destination. It has to start with a slash and include a container name (e.g. <code>/container/path/to/file.jpg</code>). The destination name always has a sequence number automatically appended (e.g. <code>.000</code>, <code>.001</code>, etc).</p>
<h3>Options</h3><p>The following command line options are available:</p>
<ul>
<li><strong><code>-b</code></strong> or <strong><code>--blocks</code></strong>: Number of blocks in each blob sent to Azure Blob Storage, each of a fixed size. The maximum (and default) value is 50,000. Setting this to a lower value can lead to more, separate blobs to be created. Because each blob has a performance target of 60MB/s, having your data split into multiple blobs allows for parallel downloads and so potentially faster restores. This has no impact on upload speed, however, as uploads are always sequential.</li>
<li><strong><code>-s</code></strong> or <strong><code>--block-size</code></strong>: Size of each block sent to Azure Blob Storage. The maximum size is 100MB, but the default value is 20MB to reduce memory footprint. Bigger block sizes allow for larger blobs: assuming 50,000 blocks per blob (the default and maximum value), with 100MB-blocks each blob can be up to ~4.8TB, while with 20MB-blocks blobs are limited to ~1TB.</li>
<li><strong><code>--endpoint</code></strong>: Endpoint to use. The default value is <code>blob.core.windows.net</code>, which is used by the global Azure infrastructure. Other common values are <code>blob.core.cloudapi.net</code> for Azure Germany and <code>blob.core.chinacloudapi.cn</code> for Azure China. Users of Azure Stack can enter their custom endpoint.</li>
<li><strong><code>--no-md5</code></strong>: Skip calculating MD5 checksums locally before uploading blocks. This can speed up operation on slower systems, but offers no protection against data corruption while in transit.</li>
<li><strong><code>-h</code></strong> or <strong><code>--help</code></strong>: Prints help message</li>
<li><strong><code>-V</code></strong> or <strong><code>--version</code></strong>: Prints application version</li>
</ul>
<h3>Examples</h3><p>Set credentials:</p>
<pre class="prettyprint source lang-sh"><code># First method: use export statements (bash syntax)
$ export AZURE_STORAGE_ACCOUNT=&quot;storageaccountname&quot;
$ export AZURE_STORAGE_ACCESS_KEY=&quot;abc123&quot;
$ azbak archive.tar /bak/data01.tar

# Second method: pass arguments inline
$ AZURE_STORAGE_ACCOUNT=&quot;storageaccountname&quot; AZURE_STORAGE_ACCESS_KEY=&quot;abc123&quot; azbak archive.tar /bak/data01.tar</code></pre><p>Upload file from local disk:</p>
<pre class="prettyprint source lang-sh"><code># Upload file archive.tar to Azure Blob Storage, named &quot;path/data01.tar&quot; inside the Storage Account &quot;bak&quot;
$ azbak archive.tar /bak/path/data01.tar</code></pre><p>Stream from stdin:</p>
<pre class="prettyprint source lang-sh"><code># Syntax
$ azbak - /container/file-from-stdin.tar

# Example: gzip file and upload
$ cat largefile.dat | gzip | azbak - /bak/largefile.dat.gz</code></pre><h1>Library</h1><h2>Installation</h2><p>azbak requires Node.js version 4.0 or higher and NPM.</p>
<p>You can install the package from NPM:</p>
<pre class="prettyprint source lang-sh"><code>$ npm install --save azbak</code></pre><h2>Usage</h2><p>You can use azbak as a library for other Node.js applications.</p>
<p>Example code:</p>
<pre class="prettyprint source lang-js"><code>const StreamUpload = require('azbak')

// Create the StreamUpload object
let upload = new StreamUpload(sourceStream, destinationPath, storageAccountName, storageAccountKey)

// Pass options
upload.blockSize = 10 * 1024 * 1024

// Start upload
let uploadPromise = upload.upload()

// uploadPromise is a then-able
uploadPromise.then((urls) => {
    // List of blobs uploaded
    console.log(urls)
}, (err) => {
    // In case of errors
    console.log('Upload failed: ', err)
})</code></pre><p>Full API documentation is available in the <a href="docs">/docs</a> folder.</p></article>
    </section>






</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Classes</h3><ul><li><a href="Authorization.html">Authorization</a></li><li><a href="StreamUpload.html">StreamUpload</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc 3.4.3</a>
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>